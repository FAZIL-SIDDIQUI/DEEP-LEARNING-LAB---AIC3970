{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a531f77",
   "metadata": {},
   "source": [
    "# use symthetic data\n",
    "# keep in mind network should converge \n",
    "# use the best weight initialization strategy\n",
    "# implement backpropogation from scratch \n",
    "# loss binary cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a2af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "[[-0.43934378]\n",
      " [ 0.71422514]\n",
      " [ 2.95578809]\n",
      " [-1.03654044]\n",
      " [-1.61698721]\n",
      " [-1.00351409]\n",
      " [ 1.83080424]\n",
      " [ 0.65750222]\n",
      " [-1.05952041]\n",
      " [ 1.02653487]\n",
      " [ 0.1941551 ]\n",
      " [ 1.93728998]\n",
      " [-1.40410619]\n",
      " [-0.65532429]\n",
      " [-0.78421631]\n",
      " [-2.9270299 ]\n",
      " [ 0.59224055]\n",
      " [ 0.52211054]\n",
      " [ 0.01022691]\n",
      " [-0.46917427]]\n",
      "Initial weights:\n",
      "W1 (input -> hidden1):\n",
      "[[-0.01415371 -0.00420645 -0.00342715 -0.00802277]]\n",
      "\n",
      "W2 (hidden1 -> output):\n",
      "[[-0.00161286]\n",
      " [ 0.00404051]\n",
      " [ 0.01886186]\n",
      " [ 0.00174578]]\n",
      "Epoch 0, Loss: 0.693754\n",
      "Epoch 100, Loss: 0.679482\n",
      "Epoch 200, Loss: 0.596632\n",
      "Epoch 300, Loss: 0.424481\n",
      "Epoch 400, Loss: 0.294420\n",
      "Epoch 500, Loss: 0.216297\n",
      "Epoch 600, Loss: 0.168144\n",
      "Epoch 700, Loss: 0.136557\n",
      "Epoch 800, Loss: 0.114715\n",
      "Epoch 900, Loss: 0.098961\n",
      "\n",
      "Training accuracy: 1.0\n",
      "Test accuracy: 0.95\n",
      "\n",
      "Sample predictions:\n",
      "Input: -0.4393, True: 0, Predicted: 0.1055, Binary Prediction: 0\n",
      "Input: 0.7142, True: 1, Predicted: 0.9127, Binary Prediction: 1\n",
      "Input: 2.9558, True: 1, Predicted: 0.9699, Binary Prediction: 1\n",
      "Input: -1.0365, True: 0, Predicted: 0.0215, Binary Prediction: 0\n",
      "Input: -1.6170, True: 0, Predicted: 0.0119, Binary Prediction: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def Relu(x):\n",
    "    return np.maximum(0 ,X)\n",
    "def relu_deriv(x):\n",
    "    return (x> 0 ).astype(float)\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, input_size=1, hidden1_size=4, output_size=1, learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.hidden1_size = hidden1_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        \n",
    "        self.w1 = np.random.randn(input_size, hidden1_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden1_size))    \n",
    "        \n",
    "        # He initialization for second layer\n",
    "        self.w2 = np.random.randn(hidden1_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "        \n",
    "        self.V_w1 = np.zeros_like\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        \n",
    "        # Input to hidden layer\n",
    "        self.z1 = np.dot(X, self.w1) + self.b1\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "        \n",
    "        # Hidden layer to output\n",
    "        self.z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "        self.output = sigmoid(self.z2)\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, y):\n",
    "        m = self.X.shape[0] \n",
    "        \n",
    "        \n",
    "        epsilon = 1e-15  # to avoid log(0)\n",
    "        self.output = np.clip(self.output, epsilon, 1 - epsilon)\n",
    "        loss = -np.mean(y * np.log(self.output) + (1 - y) * np.log(1 - self.output))\n",
    "        \n",
    "        \n",
    "        delta_output = self.output - y\n",
    "        \n",
    "        # Gradients for output layer\n",
    "        delta2 = delta_output  \n",
    "        dw2 = np.dot(self.a1.T, delta2) / m\n",
    "        db2 = np.sum(delta2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Gradients for first hidden layer\n",
    "        delta1 = np.dot(delta2, self.w2.T) * sigmoid_derivative(self.a1)\n",
    "        dw1 = np.dot(self.X.T, delta1) / m\n",
    "        db1 = np.sum(delta1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Store gradients\n",
    "        self.dw1, self.db1 = dw1, db1\n",
    "        self.dw2, self.db2 = dw2, db2\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def update_weights(self):\n",
    "        \n",
    "        self.w1 -= self.learning_rate * self.dw1\n",
    "        self.b1 -= self.learning_rate * self.db1\n",
    "        \n",
    "        self.w2 -= self.learning_rate * self.dw2\n",
    "        self.b2 -= self.learning_rate * self.db2\n",
    "    \n",
    "    def train_iteration(self, X, y):\n",
    "        # One iteration of training\n",
    "        self.forward(X)\n",
    "        loss = self.backward(y)\n",
    "        self.update_weights()\n",
    "        return loss\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, verbose=True):\n",
    "        \"\"\"Train the network for multiple epochs\"\"\"\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            loss = self.train_iteration(X, y)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            if verbose and epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "                \n",
    "        return losses\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "num_samples = 100\n",
    "\n",
    "\n",
    "X = np.random.randn(num_samples, 1) * 2 \n",
    "\n",
    "y = (X > 0).astype(float)  \n",
    "print(y)\n",
    "\n",
    "train_ratio = 0.8\n",
    "train_size = int(num_samples * train_ratio)\n",
    "\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "print(X_test)\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "\n",
    "mlp = MultiLayerPerceptron(input_size=1, hidden1_size=4, output_size=1, learning_rate=0.1)\n",
    "\n",
    "# Print initial weights\n",
    "print(\"Initial weights:\")\n",
    "print(\"W1 (input -> hidden1):\")\n",
    "print(mlp.w1)\n",
    "print(\"\\nW2 (hidden1 -> output):\")\n",
    "print(mlp.w2)\n",
    "\n",
    "# Train the model\n",
    "losses = mlp.train(X_train, y_train, epochs=1000, verbose=True)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_train = mlp.forward(X_train)\n",
    "y_pred_train_binary = (y_pred_train > 0.5).astype(float)\n",
    "train_accuracy = np.mean(y_pred_train_binary == y_train)\n",
    "\n",
    "y_pred_test = mlp.forward(X_test)\n",
    "y_pred_test_binary = (y_pred_test > 0.5).astype(float)\n",
    "test_accuracy = np.mean(y_pred_test_binary == y_test)\n",
    "\n",
    "print(\"\\nTraining accuracy:\", train_accuracy)\n",
    "print(\"Test accuracy:\", test_accuracy)\n",
    "\n",
    "# Display some sample predictions\n",
    "print(\"\\nSample predictions:\")\n",
    "for i in range(5):\n",
    "    x_sample = X_test[i]\n",
    "    y_true = y_test[i]\n",
    "    y_pred = mlp.forward(x_sample.reshape(1, -1))[0][0]\n",
    "    \n",
    "    print(f\"Input: {x_sample[0]:.4f}, True: {int(y_true[0])}, Predicted: {y_pred:.4f}, Binary Prediction: {int(y_pred > 0.5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1eaebb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.808286\n",
      "Epoch 100, Loss: 0.371731\n",
      "Epoch 200, Loss: 0.132664\n",
      "Epoch 300, Loss: 0.078281\n",
      "Epoch 400, Loss: 0.060665\n",
      "Epoch 500, Loss: 0.056182\n",
      "Epoch 600, Loss: 0.039847\n",
      "Epoch 700, Loss: 0.035158\n",
      "Epoch 800, Loss: 0.032103\n",
      "Epoch 900, Loss: 0.029068\n",
      "Accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, input_size=1, hidden1_size=4, output_size=1, learning_rate=0.01, optimizer='adam'):\n",
    "        self.input_size = input_size\n",
    "        self.hidden1_size = hidden1_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer.lower()\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.w1 = np.random.randn(input_size, hidden1_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden1_size))    \n",
    "        self.w2 = np.random.randn(hidden1_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "        # Common velocity terms\n",
    "        self.v_w1 = np.zeros_like(self.w1)\n",
    "        self.v_b1 = np.zeros_like(self.b1)\n",
    "        self.v_w2 = np.zeros_like(self.w2)\n",
    "        self.v_b2 = np.zeros_like(self.b2)\n",
    "\n",
    "        # For AdaGrad, RMSProp, Adam\n",
    "        self.eps = 1e-8\n",
    "        self.G_w1 = np.zeros_like(self.w1)\n",
    "        self.G_b1 = np.zeros_like(self.b1)\n",
    "        self.G_w2 = np.zeros_like(self.w2)\n",
    "        self.G_b2 = np.zeros_like(self.b2)\n",
    "\n",
    "        # Adam specific\n",
    "        self.m_w1 = np.zeros_like(self.w1)\n",
    "        self.vv_w1 = np.zeros_like(self.w1)\n",
    "        self.m_b1 = np.zeros_like(self.b1)\n",
    "        self.vv_b1 = np.zeros_like(self.b1)\n",
    "        self.m_w2 = np.zeros_like(self.w2)\n",
    "        self.vv_w2 = np.zeros_like(self.w2)\n",
    "        self.m_b2 = np.zeros_like(self.b2)\n",
    "        self.vv_b2 = np.zeros_like(self.b2)\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.t = 0\n",
    "\n",
    "        # RMSProp\n",
    "        self.decay_rate = 0.9\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.z1 = np.dot(X, self.w1) + self.b1\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "        self.output = sigmoid(self.z2)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, y):\n",
    "        m = self.X.shape[0]\n",
    "        output_clipped = np.clip(self.output, 1e-15, 1 - 1e-15)\n",
    "        loss = -np.mean(y * np.log(output_clipped) + (1 - y) * np.log(1 - output_clipped))\n",
    "\n",
    "        delta_output = self.output - y\n",
    "        dw2 = np.dot(self.a1.T, delta_output) / m\n",
    "        db2 = np.sum(delta_output, axis=0, keepdims=True) / m\n",
    "        delta1 = np.dot(delta_output, self.w2.T) * sigmoid_derivative(self.a1)\n",
    "        dw1 = np.dot(self.X.T, delta1) / m\n",
    "        db1 = np.sum(delta1, axis=0, keepdims=True) / m\n",
    "\n",
    "        return loss, dw1, db1, dw2, db2\n",
    "\n",
    "    def update_weights(self, dw1, db1, dw2, db2):\n",
    "        if self.optimizer == 'momentum':\n",
    "            gamma = 0.9\n",
    "            self.v_w1 = gamma * self.v_w1 + self.learning_rate * dw1\n",
    "            self.v_b1 = gamma * self.v_b1 + self.learning_rate * db1\n",
    "            self.v_w2 = gamma * self.v_w2 + self.learning_rate * dw2\n",
    "            self.v_b2 = gamma * self.v_b2 + self.learning_rate * db2\n",
    "            self.w1 -= self.v_w1\n",
    "            self.b1 -= self.v_b1\n",
    "            self.w2 -= self.v_w2\n",
    "            self.b2 -= self.v_b2\n",
    "\n",
    "        elif self.optimizer == 'nag':\n",
    "            gamma = 0.9\n",
    "            # Look ahead step\n",
    "            w1_ahead = self.w1 - gamma * self.v_w1\n",
    "            b1_ahead = self.b1 - gamma * self.v_b1\n",
    "            w2_ahead = self.w2 - gamma * self.v_w2\n",
    "            b2_ahead = self.b2 - gamma * self.v_b2\n",
    "\n",
    "            z1 = np.dot(self.X, w1_ahead) + b1_ahead\n",
    "            a1 = sigmoid(z1)\n",
    "            z2 = np.dot(a1, w2_ahead) + b2_ahead\n",
    "            output = sigmoid(z2)\n",
    "            delta_output = output - self.y_batch\n",
    "            dw2 = np.dot(a1.T, delta_output) / self.X.shape[0]\n",
    "            db2 = np.sum(delta_output, axis=0, keepdims=True) / self.X.shape[0]\n",
    "            delta1 = np.dot(delta_output, w2_ahead.T) * sigmoid_derivative(a1)\n",
    "            dw1 = np.dot(self.X.T, delta1) / self.X.shape[0]\n",
    "            db1 = np.sum(delta1, axis=0, keepdims=True) / self.X.shape[0]\n",
    "\n",
    "            # Update\n",
    "            self.v_w1 = gamma * self.v_w1 + self.learning_rate * dw1\n",
    "            self.v_b1 = gamma * self.v_b1 + self.learning_rate * db1\n",
    "            self.v_w2 = gamma * self.v_w2 + self.learning_rate * dw2\n",
    "            self.v_b2 = gamma * self.v_b2 + self.learning_rate * db2\n",
    "            self.w1 -= self.v_w1\n",
    "            self.b1 -= self.v_b1\n",
    "            self.w2 -= self.v_w2\n",
    "            self.b2 -= self.v_b2\n",
    "\n",
    "        elif self.optimizer == 'adagrad':\n",
    "            self.G_w1 += dw1 ** 2\n",
    "            self.G_b1 += db1 ** 2\n",
    "            self.G_w2 += dw2 ** 2\n",
    "            self.G_b2 += db2 ** 2\n",
    "            self.w1 -= self.learning_rate * dw1 / (np.sqrt(self.G_w1) + self.eps)\n",
    "            self.b1 -= self.learning_rate * db1 / (np.sqrt(self.G_b1) + self.eps)\n",
    "            self.w2 -= self.learning_rate * dw2 / (np.sqrt(self.G_w2) + self.eps)\n",
    "            self.b2 -= self.learning_rate * db2 / (np.sqrt(self.G_b2) + self.eps)\n",
    "\n",
    "        elif self.optimizer == 'rmsprop':\n",
    "            self.G_w1 = self.decay_rate * self.G_w1 + (1 - self.decay_rate) * (dw1 ** 2)\n",
    "            self.G_b1 = self.decay_rate * self.G_b1 + (1 - self.decay_rate) * (db1 ** 2)\n",
    "            self.G_w2 = self.decay_rate * self.G_w2 + (1 - self.decay_rate) * (dw2 ** 2)\n",
    "            self.G_b2 = self.decay_rate * self.G_b2 + (1 - self.decay_rate) * (db2 ** 2)\n",
    "            self.w1 -= self.learning_rate * dw1 / (np.sqrt(self.G_w1) + self.eps)\n",
    "            self.b1 -= self.learning_rate * db1 / (np.sqrt(self.G_b1) + self.eps)\n",
    "            self.w2 -= self.learning_rate * dw2 / (np.sqrt(self.G_w2) + self.eps)\n",
    "            self.b2 -= self.learning_rate * db2 / (np.sqrt(self.G_b2) + self.eps)\n",
    "\n",
    "        elif self.optimizer == 'adam':\n",
    "            self.t += 1\n",
    "            self.m_w1 = self.beta1 * self.m_w1 + (1 - self.beta1) * dw1\n",
    "            self.vv_w1 = self.beta2 * self.vv_w1 + (1 - self.beta2) * (dw1 ** 2)\n",
    "            m_hat_w1 = self.m_w1 / (1 - self.beta1 ** self.t)\n",
    "            v_hat_w1 = self.vv_w1 / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            self.m_b1 = self.beta1 * self.m_b1 + (1 - self.beta1) * db1\n",
    "            self.vv_b1 = self.beta2 * self.vv_b1 + (1 - self.beta2) * (db1 ** 2)\n",
    "            m_hat_b1 = self.m_b1 / (1 - self.beta1 ** self.t)\n",
    "            v_hat_b1 = self.vv_b1 / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            self.m_w2 = self.beta1 * self.m_w2 + (1 - self.beta1) * dw2\n",
    "            self.vv_w2 = self.beta2 * self.vv_w2 + (1 - self.beta2) * (dw2 ** 2)\n",
    "            m_hat_w2 = self.m_w2 / (1 - self.beta1 ** self.t)\n",
    "            v_hat_w2 = self.vv_w2 / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            self.m_b2 = self.beta1 * self.m_b2 + (1 - self.beta1) * db2\n",
    "            self.vv_b2 = self.beta2 * self.vv_b2 + (1 - self.beta2) * (db2 ** 2)\n",
    "            m_hat_b2 = self.m_b2 / (1 - self.beta1 ** self.t)\n",
    "            v_hat_b2 = self.vv_b2 / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            self.w1 -= self.learning_rate * m_hat_w1 / (np.sqrt(v_hat_w1) + self.eps)\n",
    "            self.b1 -= self.learning_rate * m_hat_b1 / (np.sqrt(v_hat_b1) + self.eps)\n",
    "            self.w2 -= self.learning_rate * m_hat_w2 / (np.sqrt(v_hat_w2) + self.eps)\n",
    "            self.b2 -= self.learning_rate * m_hat_b2 / (np.sqrt(v_hat_b2) + self.eps)\n",
    "\n",
    "        else:  # default: SGD\n",
    "            self.w1 -= self.learning_rate * dw1\n",
    "            self.b1 -= self.learning_rate * db1\n",
    "            self.w2 -= self.learning_rate * dw2\n",
    "            self.b2 -= self.learning_rate * db2\n",
    "\n",
    "    def train(self, X, y, epochs=1000, batch_size=16, verbose=True):\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.arange(X.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            epoch_loss = 0\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                end = i + batch_size\n",
    "                X_batch = X_shuffled[i:end]\n",
    "                y_batch = y_shuffled[i:end]\n",
    "\n",
    "                self.X = X_batch\n",
    "                self.y_batch = y_batch\n",
    "                self.forward(X_batch)\n",
    "                loss, dw1, db1, dw2, db2 = self.backward(y_batch)\n",
    "                self.update_weights(dw1, db1, dw2, db2)\n",
    "                epoch_loss += loss\n",
    "\n",
    "            avg_loss = epoch_loss / (X.shape[0] // batch_size)\n",
    "            losses.append(avg_loss)\n",
    "\n",
    "            if verbose and epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "        return losses\n",
    "\n",
    "# === Usage Example ===\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(100, 1)\n",
    "y = (X > 0).astype(float)\n",
    "\n",
    "mlp = MultiLayerPerceptron(input_size=1, hidden1_size=4, output_size=1, learning_rate=0.01, optimizer='nag')\n",
    "mlp.train(X, y, epochs=1000, batch_size=16, verbose=True)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = mlp.forward(X)\n",
    "acc = np.mean((y_pred > 0.5) == y)\n",
    "print(\"Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8b45e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
