{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b71a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use symthetic data\n",
    "# keep in mind network should converge \n",
    "# use the best weight initialization strategy\n",
    "# implement backpropogation from scratch \n",
    "#loss binary cross entropy\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# activation \n",
    "\n",
    "def sigmoid(X):\n",
    "    return 1/ 1+ np.exp(-X)\n",
    "\n",
    "def sigmoid_derivative(X):\n",
    "    return X *(X+1)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(sef , input_layer= 1 , hidden_layer= 4 , output_layer = 1 , learning_rate = 0.01):\n",
    "        self.input_layer = input_layer \n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.output_layer = output_layer \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        #initialize weight and biases \n",
    "     \n",
    "        #input to hidden1 \n",
    "        w1 = np.random.randn(input_layer, hidden_layer)\n",
    "        b1 = np.random,randn(1 , hidden_layer)  \n",
    "     \n",
    "        # hidden1 to output\n",
    "        w2 = np.random.randn(hidden_layer , output_layer)\n",
    "        b2 = np.random.randn(1, output_layer)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        \n",
    "        # input to hidden1\n",
    "        \n",
    "        self.z1 = np.dot(X, self.w1) + self.b1\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "        \n",
    "        # hidden1 to output\n",
    "        \n",
    "        self.z2 = np.dot(a1 , self.w2) + self.b2\n",
    "        self.output = sigmoid(self.z2)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def backward(self, y):\n",
    "        \n",
    "        n = self.X.shape[0] \n",
    "         \n",
    "        error = self.output - y\n",
    "        squared_error = np.sum(error**2) / (2 * m)\n",
    "        \n",
    "        # Compute gradients for output layer\n",
    "        delta2 = error * sigmoid_derivative(self.output)\n",
    "        dw3 = np.dot(self.a2.T, delta2) / n\n",
    "        db3 = np.sum(delta3, axis=0, keepdims=True) / m\n",
    "    \n",
    "        \n",
    "        # Compute gradients for first hidden layer\n",
    "        delta1 = np.dot(delta2, self.w2.T) * sigmoid_derivative(self.a1)\n",
    "        dw1 = np.dot(self.X.T, delta1) / m\n",
    "        db1 = np.sum(delta1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Store gradients\n",
    "        self.dw1, self.db1 = dw1, db1\n",
    "        self.dw2 , self\n",
    "        \n",
    "        return squared_error\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "         \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4413faf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
